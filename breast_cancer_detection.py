# -*- coding: utf-8 -*-
"""BreastCancerDetectionFinal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1siEFJU4G2rgEN4127bLcB7phGaQ5zoE9
"""

!pip install tensorflow
!pip install keras
!pip install opencv-python
!pip install scikit-learn
!pip install matplotlib
!pip install pandas
!pip install numpy
!pip install shutils

import os
import shutil
import kagglehub
import numpy as np
import cv2
import pandas as pd
import tensorflow as tf
import uuid
import random
import PIL
from PIL import Image
from tensorflow.keras import layers, models, optimizers, losses
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report

def load_dataset(target_path="/content/CBIS_DDSM"):
  # 1. Download the dataset (kagglehub may create a subfolder)
  source_path = kagglehub.dataset_download("awsaf49/cbis-ddsm-breast-cancer-image-dataset")

  if not os.path.exists(target_path):
      os.makedirs(target_path)

  # "/root/.cache/kagglehub/datasets/awsaf49/cbis-ddsm-breast-cancer-image-dataset"
  shutil.move(source_path, target_path)

  print("Moved dataset to:", target_path)

load_dataset()

import os
import gc
import uuid
import random
import shutil
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import PIL
from PIL import Image
import tensorflow as tf

# =========================
# 1. Load CSV Files
# =========================
def load_csv_files(data_root):
    """
    Load the CSV files from the dataset.

    Parameters:
      data_root (str): Root directory (e.g., "/content/CBIS_DDSM/1")

    Returns:
      A dictionary containing:
        - dicom_df
        - calc_train
        - calc_test
        - mass_train
        - mass_test
    """
    csv_dir = os.path.join(data_root, "csv")
    dicom_df   = pd.read_csv(os.path.join(csv_dir, "dicom_info.csv"))
    calc_train = pd.read_csv(os.path.join(csv_dir, "calc_case_description_train_set.csv"))
    calc_test  = pd.read_csv(os.path.join(csv_dir, "calc_case_description_test_set.csv"))
    mass_train = pd.read_csv(os.path.join(csv_dir, "mass_case_description_train_set.csv"))
    mass_test  = pd.read_csv(os.path.join(csv_dir, "mass_case_description_test_set.csv"))

    return {
        "dicom_df": dicom_df,
        "calc_train": calc_train,
        "calc_test": calc_test,
        "mass_train": mass_train,
        "mass_test": mass_test
    }

# =========================
# 1a. Merge Calc Data with dicom_info
# =========================
def extract_uid(path_str):
    """
    Extracts the SeriesInstanceUID from a DICOM path.
    Example: if path_str is
      "Calc-Training_P_00005_RIGHT_CC/.../000000.dcm",
    we return the second-to-last folder.
    """
    parts = path_str.split('/')
    if len(parts) >= 2:
        return parts[-2]
    return None

def merge_calc_with_dicom(calc_df, dicom_df):
    """
    Merge a calc DataFrame (calc_train or calc_test) with dicom_df based on SeriesInstanceUID.
    This replaces the original DICOM path with the corresponding JPEG image path.
    """
    calc_df["SeriesInstanceUID"] = calc_df["image file path"].apply(extract_uid)
    merged_df = pd.merge(calc_df, dicom_df, on="SeriesInstanceUID", how="left")
    return merged_df

# =========================
# 2. Process DICOM Paths
# =========================
def process_dicom_paths(dicom_df, data_root):
    """
    Extract image path series for cropped images, full mammogram images, and ROI masks.
    Replace the old path substring with the correct directory.

    Parameters:
      dicom_df (DataFrame): The DICOM information DataFrame.
      data_root (str): Root directory ("/content/CBIS_DDSM/1")

    Returns:
      Three Pandas Series: cropped_images, full_mammogram, roi_mask
    """
    cropped_images   = dicom_df[dicom_df.SeriesDescription=="cropped images"].image_path
    full_mammogram   = dicom_df[dicom_df.SeriesDescription=="full mammogram images"].image_path
    roi_mask         = dicom_df[dicom_df.SeriesDescription=="ROI mask images"].image_path

    correct_dir = os.path.join(data_root, "jpeg")
    cropped_images = cropped_images.replace("CBIS-DDSM/jpeg", correct_dir, regex=True)
    full_mammogram = full_mammogram.replace("CBIS-DDSM/jpeg", correct_dir, regex=True)
    roi_mask = roi_mask.replace("CBIS-DDSM/jpeg", correct_dir, regex=True)

    return cropped_images, full_mammogram, roi_mask

def plot_samples(sample_series, n=5, figsize=(15,5)):
    """
    Plot n sample images from a Pandas Series.
    """
    plt.figure(figsize=figsize)
    for i, file in enumerate(sample_series.head(n)):
        try:
            with PIL.Image.open(file) as im:
                gray_img = im.convert("L")
                plt.subplot(1, n, i+1)
                plt.imshow(gray_img, cmap='gray')
                plt.axis('off')
        except Exception as e:
            print(f"Error opening {file}: {e}")
    plt.tight_layout()
    plt.show()

# =========================
# 3. Build Image Dictionaries
# =========================
def get_image_file_dict(data_series):
    """
    Build a dictionary mapping the UUID folder (index 5 of the split path)
    to the full path.
    """
    image_dict = {}
    for path_str in data_series:
        parts = path_str.split('/')
        if len(parts) > 5:
            key = parts[5]  # Use index 5 to get the UUID folder.
            image_dict[key] = path_str
    print(f"Dictionary built with {len(image_dict)} keys.")
    return image_dict

# =========================
# 4. Fix Image Paths in DataFrame
# =========================
def fix_image_paths_in_df(df, full_dict, cropped_dict, roi_dict):
    """
    Update the DataFrame in-place so that the image path columns point to the correct JPEG paths.
    It uses the key from index 5 of the split path and then prepends the expected prefix
    "/content/CBIS_DDSM/1/" if it is not already present.

    Assumes that the DataFrame columns for image paths are at index positions:
      - 11: image_file_path
      - 12: cropped_image_file_path
      - 13: ROI_mask_file_path
    """
    # Define the expected prefix.
    expected_prefix = "/content/CBIS_DDSM/1/"

    for indx, row in df.iterrows():
        # Process full image path (column index 11)
        if isinstance(row[11], str):
            parts = row[11].split('/')
            if len(parts) > 5:
                key = parts[5]
                new_full_path = full_dict.get(key, None)
                if new_full_path is not None:
                    # Prepend the expected prefix if necessary.
                    if not new_full_path.startswith(expected_prefix):
                        new_full_path = expected_prefix + new_full_path
                    df.iat[indx, 11] = new_full_path
                else:
                    df.iat[indx, 11] = None

        # Process cropped image path (column index 12)
        if isinstance(row[12], str):
            parts = row[12].split('/')
            if len(parts) > 5:
                key = parts[5]
                new_cropped_path = cropped_dict.get(key, None)
                if new_cropped_path is not None:
                    if not new_cropped_path.startswith(expected_prefix):
                        new_cropped_path = expected_prefix + new_cropped_path
                    df.iat[indx, 12] = new_cropped_path
                else:
                    df.iat[indx, 12] = None

        # Process ROI mask path (column index 13)
        if isinstance(row[13], str):
            parts = row[13].split('/')
            if len(parts) > 5:
                key = parts[5]
                new_roi_path = roi_dict.get(key, None)
                if new_roi_path is not None:
                    if not new_roi_path.startswith(expected_prefix):
                        new_roi_path = expected_prefix + new_roi_path
                    df.iat[indx, 13] = new_roi_path
                else:
                    df.iat[indx, 13] = None

def fix_image_path(old_path, dataset_path):
    """
    Removes the leading 'CBIS-DDSM/' from old_path
    and joins the remainder with dataset_path.
    """
    # Remove 'CBIS-DDSM/' if present
    new_rel_path = old_path.replace("CBIS-DDSM/", "")
    # Join with dataset_path to form the absolute path
    new_abs_path = os.path.join(dataset_path, new_rel_path)
    return new_abs_path

# =========================
# 5. Merge and Prepare Dataset
# =========================
def merge_calc_datasets(merged_calc_train, merged_calc_test, dataset_path="/content/CBIS_DDSM/1"):
    """
    Merge the merged calc_train and calc_test DataFrames.
    """

    # Example usage on the 'image_path' column in merged_calc and merged_calc_test DataFrames:
    merged_calc_train["image_path"] = merged_calc_train["image_path"].apply(
        lambda p: fix_image_path(p, dataset_path)
    )
    merged_calc_test["image_path"] = merged_calc_test["image_path"].apply(
        lambda p: fix_image_path(p, dataset_path)
    )

    return pd.concat([merged_calc_train, merged_calc_test], axis=0)

def prepare_full_dataset(full_dataset, class_mapper):
    """
    Apply the class mapper to the pathology column to create a 'labels' column.
    Extract lists of valid image paths and corresponding labels.

    Returns:
      full_images (list), full_labels (list)
    """
    full_dataset["labels"] = full_dataset["pathology"].replace(class_mapper).infer_objects(copy=False)
    valid_df = full_dataset[full_dataset["image_path"].notna()]
    full_images = valid_df["image_path"].tolist()
    full_labels = valid_df["labels"].tolist()
    return full_images, full_labels

# =========================
# 6. Copy & Augment Images
# =========================
def images_count(destination):
    """
    Print the number of images in subfolders '0' and '1' within the destination.
    """
    dest0 = os.path.join(destination, "0")
    dest1 = os.path.join(destination, "1")
    count0 = len(os.listdir(dest0)) if os.path.exists(dest0) else 0
    count1 = len(os.listdir(dest1)) if os.path.exists(dest1) else 0
    print(f"Number of images in class 0: {count0}")
    print(f"Number of images in class 1: {count1}")

def augment_image(image):
    """
    Apply simple augmentations to the input tensor.
    """
    image = tf.image.random_flip_left_right(image)
    image = tf.image.random_brightness(image, max_delta=0.3)
    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)
    image = tf.image.random_saturation(image, lower=0.8, upper=1.2)
    return image

def resize_image(image_tensor):
    """
    Resize the image tensor to (224, 224).
    """
    return tf.image.resize(image_tensor, [224, 224])

def augment_and_save_images(images_list, destination_dir, num_augments):
    """
    For each augmentation, randomly select an image from the provided list,
    apply augmentation, and save it in the destination directory.
    """
    for i in range(num_augments):
        img_name = random.choice(images_list)
        abs_path = os.path.join(destination_dir, img_name)
        try:
            with Image.open(abs_path) as img:
                img = img.convert('RGB')
                img_tensor = tf.convert_to_tensor(img)
                aug_tensor = augment_image(img_tensor)
                aug_img = tf.keras.preprocessing.image.array_to_img(aug_tensor)
                base_name = os.path.splitext(img_name)[0]
                new_fname = f"{base_name}_aug{i}.jpg"
                aug_img.save(os.path.join(destination_dir, new_fname), 'JPEG')
        except Exception as e:
            print(f"Error augmenting image {abs_path}: {e}")

def copy_images_with_unique_filenames(images, labels, destination, target_count=None):
    """
    Copy images to destination subfolders ('0' and '1') with unique filenames.
    Optionally apply balancing/extra augmentation if target_count is set.
    """
    benign_images = 0
    malignant_images = 0
    skipped_images = []

    dest0 = os.path.join(destination, "0")
    os.makedirs(dest0, exist_ok=True)
    dest1 = os.path.join(destination, "1")
    os.makedirs(dest1, exist_ok=True)

    benign_list = []
    malignant_list = []

    for (img_path, label) in zip(images, labels):
        if not os.path.exists(img_path):
            print(f"Image not found: {img_path}")
            skipped_images.append(img_path)
            continue
        try:
            base_name = os.path.basename(img_path)
            unique_name = f"{uuid.uuid4().hex}_{base_name}"
            with Image.open(img_path) as img:
                img = img.convert("RGB")
                img_tensor = tf.convert_to_tensor(img)
                resized_tensor = resize_image(img_tensor)
                aug_tensor = augment_image(resized_tensor)
                aug_img = tf.keras.preprocessing.image.array_to_img(aug_tensor)
                if label == 0:
                    benign_list.append(unique_name)
                    save_path = os.path.join(dest0, unique_name)
                    aug_img.save(save_path, "JPEG")
                    benign_images += 1
                else:
                    malignant_list.append(unique_name)
                    save_path = os.path.join(dest1, unique_name)
                    aug_img.save(save_path, "JPEG")
                    malignant_images += 1
        except Exception as e:
            print(f"Error copying image {img_path}: {e}")
            skipped_images.append(img_path)

    # Balance classes if needed.
    b_count = len(benign_list)
    m_count = len(malignant_list)
    if b_count < m_count:
        diff = m_count - b_count
        augment_and_save_images(benign_list, dest0, diff)
        print("Balancing: augmented benign class.")
    elif m_count < b_count:
        diff = b_count - m_count
        augment_and_save_images(malignant_list, dest1, diff)
        print("Balancing: augmented malignant class.")

    # Extra augmentation if target_count is set.
    if target_count:
        augment_and_save_images(benign_list, dest0, target_count)
        augment_and_save_images(malignant_list, dest1, target_count)
        print("Extra data augmentation done.")

    print("\nCopying complete.")
    print(f"Benign images copied: {benign_images}")
    print(f"Malignant images copied: {malignant_images}")
    print(f"Skipped images: {len(skipped_images)}")
    if skipped_images:
        print("Skipped:")
        for s in skipped_images:
            print(s)

# =========================
# 7. Main Pipeline
# =========================
def main():
    # Define dataset root
    data_root = "/content/CBIS_DDSM/1"  # Your dataset directory
    # Load CSV files
    csvs = load_csv_files(data_root)
    dicom_df = csvs["dicom_df"]
    calc_train = csvs["calc_train"]
    calc_test = csvs["calc_test"]
    # Optionally, mass_train/test can be used similarly.

    # Merge calc CSV with dicom_info to get JPEG paths.
    merged_calc_train = merge_calc_with_dicom(calc_train, dicom_df)
    merged_calc_test  = merge_calc_with_dicom(calc_test, dicom_df)

    # Process DICOM paths and display samples
    cropped_images, full_mammogram, roi_mask = process_dicom_paths(dicom_df, data_root)
    print("Sample cropped image path:", cropped_images.iloc[0])
    plot_samples(cropped_images)

    # Build dictionaries for image paths (using UUID folder as key)
    cropped_dict = get_image_file_dict(cropped_images)
    full_dict    = get_image_file_dict(full_mammogram)
    roi_dict     = get_image_file_dict(roi_mask)

    # Fix image paths in merged calc datasets so that they now use the JPEG paths.
    fix_image_paths_in_df(merged_calc_train, full_dict, cropped_dict, roi_dict)
    fix_image_paths_in_df(merged_calc_test, full_dict, cropped_dict, roi_dict)

    # Rename columns (if needed) for consistency.
    rename_cols = {
        'left or right breast': 'left_or_right_breast',
        'image view': 'image_view',
        'abnormality id': 'abnormality_id',
        'abnormality type': 'abnormality_type',
        'mass shape': 'mass_shape',
        'mass margins': 'mass_margins',
        'image file path': 'image_file_path',
        'cropped image file path': 'cropped_image_file_path',
        'ROI mask file path': 'ROI_mask_file_path'
    }
    merged_calc_train = merged_calc_train.rename(columns=rename_cols)
    merged_calc_test  = merged_calc_test.rename(columns=rename_cols)

    # Merge the calc datasets
    full_dataset = merge_calc_datasets(merged_calc_train, merged_calc_test)


    # Prepare full dataset: use the "image_path" column from dicom_info merge.
    class_mapper = {"MALIGNANT": 1, "BENIGN": 0, "BENIGN_WITHOUT_CALLBACK": 0}
    full_images, full_labels = prepare_full_dataset(full_dataset, class_mapper)
    print("Total valid images:", len(full_images))
    print(f"FULL IMAGES: {full_images}")

    # Define destination for merged images inside the dataset root.
    destination_dir = os.path.join(data_root, "merged_images")
    os.makedirs(destination_dir, exist_ok=True)

    # Copy images into merged folder with unique filenames and augment as needed.
    target_count = int((len(full_labels) * 5) // 2)
    copy_images_with_unique_filenames(full_images, full_labels, destination_dir, target_count=target_count)
    images_count(destination_dir)

main()

import tensorflow as tf

data_dir = "/content/CBIS_DDSM/1/merged_images"

# Create a dataset for the entire data
full_dataset = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir,
    labels='inferred',          # Infer labels from subfolder names
    label_mode='categorical',   # 2D one-hot labels for classification
    image_size=(224, 224),      # Resize all images to 224x224
    batch_size=13,              # Batch size
    seed=30,                    # For reproducible shuffling
    shuffle=True
)

# Check how many batches we have in total
total_samples = tf.data.experimental.cardinality(full_dataset).numpy()
print("Total number of batches in the full dataset:", total_samples)

# Step 3: Split the dataset into train, validation, and test sets
train_size = int(0.7 * total_samples)                 # 70% for training
test_size = total_samples - train_size                # 30% for testing

# Create train, validation, and test datasets
train_dataset       = full_dataset.take(train_size)
test_dataset        = full_dataset.skip(train_size)

# Step 5: Prefetch to Improve Performance
train_dataset      = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
test_dataset       = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)

print(f"Train samples:      {train_size}     batches(8) ==> {train_size*13}")
print(f"Test samples:       {test_size}      batches(8) ==> {test_size*13}")

import numpy as np
import gc
from tensorflow.keras.applications import (ResNet50,
                                           Xception,
                                           VGG19,
                                           InceptionV3,
                                           DenseNet121,
                                           NASNetMobile,
                                           MobileNetV2,
                                           MobileNet,
                                           EfficientNetV2B0,
                                           EfficientNetV2S,
                                           EfficientNetV2L,
                                           ConvNeXtBase)
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout
from tensorflow.keras.models import Model

def create_model(model_name, trainable_layers, dropout_value, save_weights=False):
    """
    Create a transfer learning model using a pretrained architecture.

    Args:
        model_name (str): Name of the pretrained model to use. Supported models:
            "ResNet50", "Xception", "VGG19", "InceptionV3", "DenseNet121",
            "NASNetMobile", "MobileNetV2", "MobileNet", "EfficientNetV2B0",
            "EfficientNetV2S", "EfficientNetV2L", "ConvNeXtBase".
        trainable_layers (float): Percentage of layers (from the end) to unfreeze.
                                  For example, if trainable_layers = 20, then the last
                                  20% of the layers will be set to trainable.
        dropout_value (float): Dropout rate to use in the top layers.
        save_weights (bool): If False, pretrained weights ('imagenet') are loaded;
                             otherwise, no weights are loaded.

    Returns:
        model (tf.keras.Model): The compiled model with a new classifier on top.
    """
    model_dict = {
        "ResNet50":         ResNet50,
        "Xception":         Xception,
        "VGG19":            VGG19,
        "InceptionV3":      InceptionV3,
        "DenseNet121":      DenseNet121,
        "NASNetMobile":     NASNetMobile,
        "MobileNetV2":      MobileNetV2,
        "MobileNet":        MobileNet,
        "EfficientNetV2B0": EfficientNetV2B0,
        "EfficientNetV2S":  EfficientNetV2S,
        "EfficientNetV2L":  EfficientNetV2L,
        "ConvNeXtBase":     ConvNeXtBase,
    }

    if model_name not in model_dict:
        raise ValueError(f"Model {model_name} is not supported.")

    # Load the base model without its top layers.
    base_model = model_dict[model_name](
        weights='imagenet' if not save_weights else None,
        include_top=False,
        input_shape=(224, 224, 3)
    )

    # Freeze all layers in the base model.
    for layer in base_model.layers:
        layer.trainable = False

    # Calculate the index to start unfreezing layers.
    # For example, if trainable_layers=20, unfreeze the last 20% of the layers.
    from_index = int(np.round((len(base_model.layers) - 1) * (1.0 - trainable_layers / 100.0)))

    # Unfreeze layers from the calculated index onward.
    for layer in base_model.layers[from_index:]:
        layer.trainable = True

    # Add custom top layers.
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(1024, activation='relu')(x)
    x = Dropout(dropout_value)(x)
    predictions = Dense(2, activation='softmax')(x)  # Assuming binary classification.

    model = Model(inputs=base_model.input, outputs=predictions)

    return model

import os
import gc
import numpy as np
import tensorflow as tf
import cv2
from tensorflow.keras import Model
from tensorflow.keras.layers import Input, Conv2D, GlobalAveragePooling2D, Dense, Dropout

# Helper function to build Gabor filters
def build_gabor_filters(kernel_size, num_filters=8):
    filters = []
    # You can tune these parameters (sigma, theta, lambda, gamma, psi) as needed
    sigma = 4.0
    lambd = 10.0
    gamma = 0.5
    psi = 0
    for theta in np.linspace(0, np.pi, num_filters, endpoint=False):
        kern = cv2.getGaborKernel((kernel_size, kernel_size), sigma, theta, lambd, gamma, psi, ktype=cv2.CV_32F)
        # Normalize kernel (optional)
        kern /= 1.5 * kern.sum()
        filters.append(kern)
    filters = np.stack(filters, axis=-1)  # Shape: (kernel_size, kernel_size, num_filters)
    # Expand dims for input channels; we initially create filters for 1 channel.
    filters = np.expand_dims(filters, axis=-2)  # Now shape: (kernel_size, kernel_size, 1, num_filters)
    return filters

# Define a custom Gabor layer
class GaborLayer(tf.keras.layers.Layer):
    def __init__(self, kernel_size=7, num_filters=8, **kwargs):
        super(GaborLayer, self).__init__(**kwargs)
        self.kernel_size = kernel_size
        self.num_filters = num_filters

    def build(self, input_shape):
        # input_shape: (batch, height, width, channels)
        input_channels = input_shape[-1]
        # Build Gabor filters for 1 channel and repeat for all input channels
        filters = build_gabor_filters(self.kernel_size, self.num_filters)
        filters = np.repeat(filters, input_channels, axis=2)
        self.gabor_filters = tf.constant(filters, dtype=tf.float32)
        super(GaborLayer, self).build(input_shape)

    def call(self, inputs):
        # Apply 2D convolution using the fixed Gabor filters
        outputs = tf.nn.conv2d(inputs, self.gabor_filters, strides=[1, 1, 1, 1], padding='SAME')
        return outputs

# Now, let's integrate the Gabor layer with the base model.
def create_model_with_gabor(model_name, trainable_layers, dropout_value, save_weights=False):
    # Create the base model (expects input shape (224, 224, 3))
    base_model = create_model(model_name, trainable_layers, dropout_value, save_weights=save_weights)

    # Define a new input layer
    inputs = Input(shape=(224, 224, 3))
    # Apply the Gabor layer
    x = GaborLayer(kernel_size=7, num_filters=8)(inputs)
    # To match the input shape expected by the base model, project the output to 3 channels using a 1x1 convolution
    x = Conv2D(3, kernel_size=1, activation='relu', padding='same')(x)
    # Pass the output to the base model
    outputs = base_model(x)

    model = Model(inputs, outputs)
    return model

import os
import gc
import numpy as np
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping, CSVLogger, ModelCheckpoint, TerminateOnNaN
from tensorflow.keras.optimizers import Adam  # or any other optimizer you want to use

# Assume WIDTH, PROJECT_PATH, make_file, reset_memory, and train_dataset are defined
# For instance:
WIDTH = 80
PROJECT_PATH = "/content/CBIS_DDSM/1"

def make_file():
    os.makedirs(os.path.join(PROJECT_PATH, "Checkpoints"), exist_ok=True)
    os.makedirs(os.path.join(PROJECT_PATH, "Logs"), exist_ok=True)

def reset_memory():
    tf.keras.backend.clear_session()
    gc.collect()

def run_model(trainable_layers,
              optimizer_class,
              batch_size_value,
              dropout_value,
              model_name=None,
              save=False,
              epochs=5,
              patience=2):

    if model_name is None:
        models = ["ConvNeXtBase"]
        # Alternatively, you could use "ResNet50" or any other default model.
    else:
        models = [model_name]  # Train only the specific model passed in

    best_score = float('-inf')  # Initialize best score to negative infinity
    best_model_info = None      # To store the best model information

    for model_name in models:
        # Clear any existing Keras session to free up memory
        tf.keras.backend.clear_session()
        gc.collect()  # Ensure no memory leaks

        # Create the model using the create_model function
        # model = create_model(model_name, trainable_layers, dropout_value)
        # Example usage:
        model = create_model_with_gabor("ConvNeXtBase", trainable_layers=5, dropout_value=0.2)
        optimizer_value = optimizer_class()  # Instantiate the optimizer

        # Compile the model
        model.compile(optimizer=optimizer_value,
                      loss='categorical_crossentropy',
                      metrics=['accuracy',
                               tf.keras.metrics.Precision(name='precision'),
                               tf.keras.metrics.Recall(name='recall')])

        # Count trainable parameters
        trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])
        output = (f"Training model: {model_name}   "
                  f"Layers: {len(model.layers)}   "
                  f"Parameters: {model.count_params()}   "
                  f"Trainable parameters: {trainable_params}")
        print(output.center(WIDTH))
        print(f'#%% {"-" * 97} %'.center(WIDTH))

        # Prepare file paths if saving is enabled
        if save:
            solution = [trainable_layers, optimizer_value.__class__.__name__, batch_size_value, dropout_value]
            keyword = f"{model_name}-" + "-".join([str(el) for el in solution])
            checkpointPath = os.path.join(PROJECT_PATH, "Checkpoints", keyword) + ".weights.h5"
            csvLogPath = os.path.join(PROJECT_PATH, "Logs", keyword) + ".csv"
            make_file()  # Create necessary directories

        # Set common callbacks
        callbacks = [
            TerminateOnNaN(),
            EarlyStopping(monitor="accuracy", mode="max", patience=patience)
        ]

        # Add saving-related callbacks if saving is enabled
        if save:
            callbacks += [
                ModelCheckpoint(checkpointPath,
                                save_best_only=True,
                                save_weights_only=True,
                                monitor="accuracy",
                                mode="max",
                                verbose=0),
                CSVLogger(csvLogPath, append=True)
            ]

        # Train the model
        history = model.fit(
            train_dataset,
            batch_size=batch_size_value,
            epochs=epochs,
            callbacks=callbacks
        )

        if save:
            return model, history

        # Retrieve the final training accuracy as score
        score = history.history['accuracy'][-1]
        # Alternatively, you could use a combination of metrics (e.g., average of accuracy, precision, recall)

        # Update best model information if current score is higher
        if score > best_score:
            best_score = score
            best_model_info = {
                'model_name': model_name,
                'trainable_layers': trainable_layers,
                'optimizer': optimizer_value.__class__.__name__,
                'batch_size': batch_size_value,
                'dropout_value': dropout_value
            }

        # Clear the session after model training to free up memory
        reset_memory()
        print(f"Model {model_name} cleared from memory".center(WIDTH))
        print()

    # Optionally, you could return the best model information along with the model and history
    # For now, we return the last model trained (or you could modify to return the best model)
    return model

# =========================
# Additional Utility Functions
# =========================

def compare_model_performance(train_dataset, test_dataset,
                              trainable_layers, optimizer_class,
                              batch_size_value, dropout_value,
                              epochs, patience):
    """
    Train and compare the performance of:
      1. Normal ConvNeXt pretrained model.
      2. Gabor-enhanced ConvNeXt pretrained model.

    Both models are trained on the provided train_dataset and evaluated on test_dataset.
    The function prints and returns the evaluation metrics.
    """
    # --- Normal ConvNeXt Model ---
    print("Building and training the normal ConvNeXt model...")
    normal_model = create_model("ConvNeXtBase", trainable_layers, dropout_value, save_weights=True)
    normal_model.compile(optimizer=optimizer_class(),
                         loss='categorical_crossentropy',
                         metrics=['accuracy',
                                  tf.keras.metrics.Precision(name='precision'),
                                  tf.keras.metrics.Recall(name='recall')])

    history_normal = normal_model.fit(train_dataset,
                                      epochs=epochs,
                                      batch_size=batch_size_value,
                                      callbacks=[tf.keras.callbacks.EarlyStopping(monitor="accuracy", patience=patience)],
                                      verbose=1)
    normal_metrics = normal_model.evaluate(test_dataset, verbose=0)

    # --- Gabor-enhanced ConvNeXt Model ---
    print("\nBuilding and training the Gabor enhanced ConvNeXt model...")
    gabor_model = create_model_with_gabor("ConvNeXtBase", trainable_layers, dropout_value, save_weights=True)
    gabor_model.compile(optimizer=optimizer_class(),
                         loss='categorical_crossentropy',
                         metrics=['accuracy',
                                  tf.keras.metrics.Precision(name='precision'),
                                  tf.keras.metrics.Recall(name='recall')])

    history_gabor = gabor_model.fit(train_dataset,
                                    epochs=epochs,
                                    batch_size=batch_size_value,
                                    callbacks=[tf.keras.callbacks.EarlyStopping(monitor="accuracy", patience=patience)],
                                    verbose=1)
    gabor_metrics = gabor_model.evaluate(test_dataset, verbose=0)

    # --- Print Comparison ---
    print("\n--- Comparison of Performance ---")
    print("Normal ConvNeXt model:")
    print("   Loss: {:.4f}, Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}".format(*normal_metrics))
    print("Gabor Enhanced ConvNeXt model:")
    print("   Loss: {:.4f}, Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}".format(*gabor_metrics))

    return (normal_model, history_normal, normal_metrics), (gabor_model, history_gabor, gabor_metrics)


def save_trained_model(model, file_path):
    """
    Save the entire trained model in .h5 format.

    Args:
      model (tf.keras.Model): The trained model.
      file_path (str): The file path where the model should be saved (e.g., 'my_model.h5').
    """
    model.save(file_path)
    print(f"Model saved to {file_path}")


def load_trained_model(file_path):
    """
    Load a trained model from a .h5 file.

    Args:
      file_path (str): The file path from which to load the model.

    Returns:
      tf.keras.Model: The loaded model.

    Note: If the model contains custom layers (e.g., GaborLayer),
          pass them via custom_objects.
    """
    loaded_model = tf.keras.models.load_model(file_path, custom_objects={'GaborLayer': GaborLayer})
    print(f"Model loaded from {file_path}")
    return loaded_model

# Example call to run_model:
model, history = run_model(
    trainable_layers=5,
    optimizer_class=Adam,
    batch_size_value=8,
    dropout_value=0,
    model_name=None,   # Defaults to using "ConvNeXtBase" if None is passed
    save=True,         # Save the model weights and logs if True
    epochs=10,
    patience=2
)

save_trained_model(model, '/content/ConvNeXt_model.h5')

# Evaluate on test dataset
test_loss, test_accuracy, test_precision, test_recall = model.evaluate(test_dataset)
print(f"\nTest Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")
print(f"Test Precision: {test_precision:.4f}")
print(f"Test Recall: {test_recall:.4f}")

# Plot training curves
plt.figure(figsize=(14,5))
epochs_range = range(len(history.history['accuracy']))

# Accuracy
plt.subplot(1,2,1)
plt.plot(epochs_range, history.history['accuracy'], 'b-o', label='Train Accuracy')
plt.title("Training Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()

# Loss
plt.subplot(1,2,2)
plt.plot(epochs_range, history.history['loss'], 'r-o', label='Train Loss')
plt.title("Training Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

# Make a bar chart of final test metrics
metrics = ['Accuracy','Precision','Recall','Loss']
vals = [test_accuracy, test_precision, test_recall, test_loss]

plt.figure(figsize=(7,5))
bars = plt.bar(metrics, vals, color=['#003049', '#219ebc', '#f77f00', '#d62828'])
plt.ylim(0, 1.0)
plt.title("Test Metrics")
for bar in bars:
    h = bar.get_height()
    plt.text(bar.get_x()+bar.get_width()/2, h+0.02, f"{h:.4f}", ha='center')
plt.show()

(normal_model, history_normal, normal_metrics), (gabor_model, history_gabor, gabor_metrics) = compare_model_performance(
    train_dataset=train_dataset,
    test_dataset=test_dataset,
    trainable_layers=5,
    optimizer_class=Adam,
    batch_size_value=8,
    dropout_value=0.2,
    epochs=10,
    patience=2
)

save_trained_model(normal_model, '/content/ConvNeXt_normal_model.h5')
save_trained_model(gabor_model, '/content/ConvNeXt_gabor_model.h5')

